ğŸ” RAGInspector
A Diagnostic Framework for Evaluating RAG + LLM Pipelines

RAGInspector is an interactive diagnostic tool that x-rays your Retrieval-Augmented Generation (RAG) pipeline to reveal where it breaks and why â€” retrieval, generation, or the full pipeline.

Unlike traditional accuracy metrics, RAGInspector separates faithfulness, retrieval relevance, and end-to-end correctness, enabling targeted fixes instead of blind tuning.

ğŸš€ Why RAGInspector?

Most RAG evaluations answer only one question:

â€œIs the output correct?â€

RAGInspector answers the harder (and more useful) questions:

â“ Was the retrieved context relevant?

â“ Was the LLM faithful to that context?

â“ Is the failure caused by retrieval, generation, or both?

â“ Which fields, documents, or schemas are breaking most often?

This tool is built for:

RAG system builders

LLM platform engineers

Document intelligence teams

FinTech / Legal / Enterprise AI use cases

ğŸ§  Core Diagnostic Framework

RAGInspector evaluates a pipeline along two orthogonal axes:

1ï¸âƒ£ Faithfulness (Generation Quality)

Did the model generate answers grounded in the retrieved source?

Faithfulness = CorrectFields / (CorrectFields + IncorrectFields)


High faithfulness â†’ Model respects context

Low faithfulness â†’ Hallucination or misuse of context

2ï¸âƒ£ Relevance (Retrieval Quality â€” Proxy-Based)

Because embedding similarity is not always available at inference time,
RAGInspector uses model-reported confidence + provenance as a proxy signal.

Relevance â‰ˆ Average Field Confidence


âš ï¸ This is a heuristic, not ground truth â€” and the UI explicitly surfaces this limitation.

3ï¸âƒ£ End-to-End Accuracy

Out of everything we expected, how much did the system actually get right?

EndToEndAccuracy = CorrectFields / TotalFields

ğŸ§© Diagnostic Interpretation Matrix
Faithfulness	Relevance	Diagnosis
High	High	âœ… Pipeline Healthy (focus on edge cases)
High	Low	âš ï¸ Retrieval Problem
Low	High	âš ï¸ Generation / Hallucination Problem
Low	Low	ğŸš¨ Entire Pipeline Broken

RAGInspector automatically classifies your pipeline into one of these states.

ğŸ–¥ï¸ Key Features
âœï¸ Field-Level Human Annotation

Mark each extracted field as Correct / Incorrect

Provide:

Expected value

Error category

Free-text reasoning

ğŸ”— Provenance-Aware Inspection

View exact:

Source text

Page number

Section ID

Character offsets

Validate grounding visually

ğŸ“Š Automatic Metrics

Faithfulness score

Retrieval relevance (proxy)

End-to-end accuracy

Error rate

Annotation progress

ğŸ§  Intelligent Error Analysis

Errors are grouped into actionable categories:

Hallucination

Context missing

Partial extraction

Schema mismatch

Interpretation errors

Source quality issues

Includes retrieval vs generation blame split.

ğŸ“¤ Exportable Evaluation Reports

One-click JSON export

Suitable for:

Offline analysis

CI evaluation

Regression tracking

Model comparisons

ğŸ“‚ Supported Input Format

RAGInspector expects raw pipeline logs, not synthetic datasets.

Schema Block
{
  "schema": {
    "type": "object",
    "properties": {
      "loan_amount": {
        "type": "string",
        "description": "Total principal amount of the loan"
      }
    }
  }
}

Response Block
{
  "success": true,
  "doc_id": "doc_123",
  "extraction": {
    "loan_amount": "USD 50,000,000"
  },
  "provenance": {
    "loan_amount": {
      "confidence": 0.85,
      "source": {
        "page_number": 2,
        "source_text": "The Borrower agrees to borrow USD 50,000,000..."
      }
    }
  }
}


Multiple schema + response pairs can be pasted or uploaded in a single run.

ğŸ› ï¸ Tech Stack

React

Tailwind CSS

Lucide Icons

Fully client-side (no backend required)

âš ï¸ Honest Limitations (By Design)

RAGInspector is opinionated and transparent:

Relevance is proxy-based, not true semantic similarity

Model confidence can be overestimated

High confidence â‰  correctness

This tool prioritizes observability over theoretical purity

These limitations are explicitly documented and surfaced in the UI.

ğŸ¯ When to Use RAGInspector

âœ… Debugging hallucinations
âœ… Evaluating new chunking strategies
âœ… Comparing prompt versions
âœ… Auditing document intelligence pipelines
âœ… Building human-verified evaluation datasets

ğŸ§ª Future Extensions (Planned)

Embedding similarity integration

Automatic regression comparison

Model-to-model evaluation

CI-friendly scoring modes

Dataset export for training evaluators

ğŸ Final Thought

You canâ€™t fix a RAG pipeline if you donâ€™t know where itâ€™s lying.

RAGInspector doesnâ€™t just score your system â€”
it tells you what broke, where, and why.